{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "b8febf02-a2f2-4991-8652-01dca5253e25",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Documento Word gerado com sucesso em: C:\\Users\\CWS\\Documents\\TCC\\Meu tcc\\clusters\\comparacao_grupos_sociais.docx\n"
     ]
    }
   ],
   "source": [
    "from docx import Document\n",
    "from docx.shared import Pt\n",
    "from docx.enum.table import WD_ALIGN_VERTICAL\n",
    "import os\n",
    "\n",
    "# Criar documento\n",
    "doc = Document()\n",
    "\n",
    "# Adicionar t√≠tulo\n",
    "titulo = doc.add_heading('Compara√ß√£o entre Grupos Sociais', level=1)\n",
    "titulo.alignment = 1  # Centralizado\n",
    "\n",
    "# Adicionar tabela\n",
    "tabela = doc.add_table(rows=1, cols=4)\n",
    "tabela.style = 'Light Shading'\n",
    "\n",
    "# Cabe√ßalhos\n",
    "cabecalhos = tabela.rows[0].cells\n",
    "cabecalhos[0].text = 'Vari√°vel'\n",
    "cabecalhos[1].text = 'Vulner√°vel üü•\\n(M√©dia: 500.03)'\n",
    "cabecalhos[2].text = 'Intermedi√°rio üü®\\n(M√©dia: 575.62)'\n",
    "cabecalhos[3].text = 'Privilegiado üü©\\n(M√©dia: 660.00)'\n",
    "\n",
    "# Dados\n",
    "dados = [\n",
    "    ['Escolaridade do Pai (Q001)', '2.64', '3.92', '5.16'],\n",
    "    ['Escolaridade da M√£e (Q002)', '2.98', '4.29', '5.34'],\n",
    "    ['Ocupa√ß√£o do Pai (Q003)', '1.59', '2.79', '3.50'],\n",
    "    ['Ocupa√ß√£o da M√£e (Q004)', '1.31', '2.63', '3.42'],\n",
    "    ['Renda Familiar (Q006)', '1.55', '3.56', '10.82'],\n",
    "    ['Computador em Casa (Q024)', '0.19', '0.71', '2.21'],\n",
    "    ['Internet em Casa (Q025)', '0.83', '0.99', '1.00'],\n",
    "    ['Escola Privada (TP_ESCOLA)', '0.03', '0.47', '0.94'],\n",
    "    ['Cor/Ra√ßa (TP_COR_RACA)', '0.19', '0.38', '0.70']\n",
    "]\n",
    "\n",
    "# Adicionar linhas\n",
    "for linha in dados:\n",
    "    celulas = tabela.add_row().cells\n",
    "    for i, valor in enumerate(linha):\n",
    "        celulas[i].text = str(valor)\n",
    "        celulas[i].vertical_alignment = WD_ALIGN_VERTICAL.CENTER\n",
    "\n",
    "# Ajustar estilo\n",
    "for row in tabela.rows:\n",
    "    for cell in row.cells:\n",
    "        paragraphs = cell.paragraphs\n",
    "        for paragraph in paragraphs:\n",
    "            for run in paragraph.runs:\n",
    "                run.font.size = Pt(10)\n",
    "\n",
    "# Definir o caminho completo para salvar\n",
    "caminho_completo = r'C:\\Users\\CWS\\Documents\\TCC\\Meu tcc\\clusters\\comparacao_grupos_sociais.docx'\n",
    "\n",
    "# Criar diret√≥rio se n√£o existir\n",
    "os.makedirs(os.path.dirname(caminho_completo), exist_ok=True)\n",
    "\n",
    "# Salvar documento\n",
    "doc.save(caminho_completo)\n",
    "print(f\"Documento Word gerado com sucesso em: {caminho_completo}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "cecdfce3-d7bd-435f-8f9e-b97c89cbc221",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting python-docx\n",
      "  Downloading python_docx-1.1.2-py3-none-any.whl.metadata (2.0 kB)\n",
      "Requirement already satisfied: lxml>=3.1.0 in c:\\users\\cws\\anaconda3\\lib\\site-packages (from python-docx) (5.2.1)\n",
      "Requirement already satisfied: typing-extensions>=4.9.0 in c:\\users\\cws\\anaconda3\\lib\\site-packages (from python-docx) (4.13.2)\n",
      "Downloading python_docx-1.1.2-py3-none-any.whl (244 kB)\n",
      "Installing collected packages: python-docx\n",
      "Successfully installed python-docx-1.1.2\n"
     ]
    }
   ],
   "source": [
    "!pip install python-docx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2322041c-9943-4d74-947f-3372396e80f4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capitais n√£o mapeadas: []\n",
      "\n",
      "M√©todo do Cotovelo sugere: 5 clusters\n",
      "M√©todo da Silhueta sugere: 2 clusters\n",
      "M√©todo Davies-Bouldin sugere: 2 clusters\n",
      "\n",
      "Compara√ß√£o de m√©todos:\n",
      "Silhueta k=2: 0.322 | Silhueta k=3: 0.216\n",
      "Rela√ß√£o de qualidade: 67.0%\n",
      "R¬≤ entre clusters k=2: -0.712 | k=3: -0.437\n",
      "Ganho explicativo: -38.6%\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'cluster_names' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 232\u001b[0m\n\u001b[0;32m    226\u001b[0m cluster_names_2 \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m    227\u001b[0m     \u001b[38;5;241m0\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mVulner√°vel\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m    228\u001b[0m     \u001b[38;5;241m1\u001b[39m: \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mPrivilegiado\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m    229\u001b[0m }\n\u001b[0;32m    231\u001b[0m \u001b[38;5;66;03m# Criar nova coluna com os nomes dos clusters\u001b[39;00m\n\u001b[1;32m--> 232\u001b[0m df_cluster[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcluster_nome\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_cluster[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(cluster_names)\n\u001b[0;32m    234\u001b[0m \u001b[38;5;66;03m# Atribuir nomes leg√≠veis\u001b[39;00m\n\u001b[0;32m    235\u001b[0m df_cluster[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcluster_nome\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m df_cluster[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcluster\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mmap(cluster_names_2)\n",
      "\u001b[1;31mNameError\u001b[0m: name 'cluster_names' is not defined"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score , calinski_harabasz_score, pairwise_distances\n",
    "import plotly.express as px\n",
    "from geobr import read_state\n",
    "from matplotlib.colors import ListedColormap\n",
    "import unicodedata\n",
    "\n",
    "# Configura√ß√µes iniciais\n",
    "plt.style.use('ggplot')\n",
    "sns.set_palette(\"Set2\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# -----------------------------\n",
    "# ETAPA 1: Carregar e preparar os dados\n",
    "# -----------------------------\n",
    "# Fun√ß√£o para remover acentos e padronizar\n",
    "def normalizar_texto(texto):\n",
    "    if pd.isnull(texto):\n",
    "        return texto\n",
    "    texto = str(texto).strip().title()\n",
    "    texto = unicodedata.normalize('NFKD', texto).encode('ASCII', 'ignore').decode('utf-8')\n",
    "    return texto\n",
    "\n",
    "# Dicion√°rio: capitais do Nordeste ‚Üí siglas dos estados\n",
    "capitais_para_uf = {\n",
    "    'S√£o Lu√≠s': 'MA',\n",
    "    'Teresina': 'PI',\n",
    "    'Fortaleza': 'CE',\n",
    "    'Natal': 'RN',\n",
    "    'Jo√£o Pessoa': 'PB',\n",
    "    'Recife': 'PE',\n",
    "    'Macei√≥': 'AL',\n",
    "    'Aracaju': 'SE',\n",
    "    'Salvador': 'BA'\n",
    "}\n",
    "\n",
    "# Ajustar o dicion√°rio tamb√©m para ter nomes sem acento\n",
    "capitais_para_uf_normalizado = {normalizar_texto(k): v for k, v in capitais_para_uf.items()}\n",
    "\n",
    "# Carregar dados do ENEM (ajuste o caminho)\n",
    "caminho_arquivo = r\"C:\\Users\\CWS\\Documents\\meu\\Bases clustering\\microdados_enem_tratados_5.csv\"\n",
    "df = pd.read_csv(caminho_arquivo, sep=\";\", encoding=\"utf-8\")\n",
    "\n",
    "df['NO_MUNICIPIO_PROVA_NORMALIZADO'] = df['NO_MUNICIPIO_ESC'].apply(normalizar_texto)\n",
    "\n",
    "# Mapeia sigla da UF a partir da capital normalizada\n",
    "capitais_para_uf_normalizado = {normalizar_texto(k): v for k, v in capitais_para_uf.items()}\n",
    "df['SG_UF_ESC'] = df['NO_MUNICIPIO_PROVA_NORMALIZADO'].map(capitais_para_uf_normalizado)\n",
    "\n",
    "faltantes = df[df['SG_UF_ESC'].isnull()]['NO_MUNICIPIO_ESC'].unique()\n",
    "print(\"Capitais n√£o mapeadas:\", faltantes)\n",
    "\n",
    "# Filtrar apenas alunos do Nordeste\n",
    "ufs_nordeste = ['MA', 'PI', 'CE', 'RN', 'PB', 'PE', 'AL', 'SE', 'BA']\n",
    "df_nordeste = df[df['SG_UF_ESC'].isin(ufs_nordeste)].copy()\n",
    "\n",
    "# Calcular nota m√©dia\n",
    "df_nordeste['NU_NOTA_MEDIA'] = df_nordeste[['NU_NOTA_CN', 'NU_NOTA_CH', 'NU_NOTA_LC', 'NU_NOTA_MT', 'NU_NOTA_REDACAO']].mean(axis=1)\n",
    "\n",
    "# Selecionar vari√°veis para clusteriza√ß√£o\n",
    "variaveis = [\n",
    "    'Q001',  # Escolaridade do pai\n",
    "    'Q002',  # Escolaridade da m√£e\n",
    "    'Q003',  # Ocupa√ß√£o do pai\n",
    "    'Q004',  # Ocupa√ß√£o da m√£e\n",
    "    'Q006',  # Renda familiar\n",
    "    'Q024',  # Computador em casa\n",
    "    'Q025',  # Internet em casa\n",
    "    'TP_ESCOLA',  # Tipo de escola\n",
    "    'TP_COR_RACA',  # Ra√ßa/cor\n",
    "    'NU_NOTA_MEDIA'  # Desempenho acad√™mico\n",
    "]\n",
    "\n",
    "df_cluster = df_nordeste[variaveis].dropna().copy()\n",
    "\n",
    "# -----------------------------\n",
    "# ETAPA 2: Pr√©-processamento\n",
    "# -----------------------------\n",
    "\n",
    "# Codificar vari√°veis categ√≥ricas\n",
    "encoder = LabelEncoder()\n",
    "df_cluster['TP_ESCOLA'] = encoder.fit_transform(df_cluster['TP_ESCOLA'])\n",
    "df_cluster['TP_COR_RACA'] = encoder.fit_transform(df_cluster['TP_COR_RACA'])\n",
    "\n",
    "\n",
    "# Normalizar os dados\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(df_cluster)\n",
    "\n",
    "# -----------------------------\n",
    "# ETAPA 3: Determinar n√∫mero √≥timo de clusters\n",
    "\n",
    "range_clusters = range(1, 11)  # Definir o range de clusters a testar\n",
    "# -----------------------------\n",
    "\n",
    "# M√©todo do cotovelo (melhorado)\n",
    "wcss = []\n",
    "for i in range_clusters:\n",
    "    kmeans = KMeans(n_clusters=i, random_state=42)\n",
    "    kmeans.fit(X)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "# Identificar o \"cotovelo\" programaticamente\n",
    "elbow_diff = np.diff(wcss)\n",
    "elbow_angle = np.diff(elbow_diff)\n",
    "optimal_elbow = np.argmin(elbow_angle) + 2  # +2 porque diff reduz em 2 o tamanho\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range_clusters, wcss, marker='o', label='WCSS')\n",
    "plt.axvline(x=optimal_elbow, color='r', linestyle='--', \n",
    "            label=f'Cotovelo sugerido (k={optimal_elbow})')\n",
    "plt.title('M√©todo do Cotovelo - N√∫mero √ìtimo de Clusters', fontsize=14)\n",
    "plt.xlabel('N√∫mero de Clusters')\n",
    "plt.ylabel('WCSS (In√©rcia)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(r\"C:\\Users\\CWS\\Documents\\meu\\Meu tcc\\clusters\\elbow_method_improved.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(f\"\\nM√©todo do Cotovelo sugere: {optimal_elbow} clusters\")\n",
    "\n",
    "# M√©todo da silhueta (melhorado)\n",
    "silhouette_scores = []\n",
    "range_silhouette = range(2, 11)  # Silhueta requer pelo menos 2 clusters\n",
    "\n",
    "for i in range_silhouette:\n",
    "    kmeans = KMeans(n_clusters=i, random_state=42)\n",
    "    labels = kmeans.fit_predict(X)\n",
    "    silhouette_scores.append(silhouette_score(X, labels))\n",
    "\n",
    "optimal_silhouette = range_silhouette[np.argmax(silhouette_scores)]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range_silhouette, silhouette_scores, marker='o', label='Score de Silhueta')\n",
    "plt.axvline(x=optimal_silhouette, color='g', linestyle='--', \n",
    "            label=f'√ìtimo sugerido (k={optimal_silhouette})')\n",
    "plt.title('M√©todo da Silhueta - N√∫mero √ìtimo de Clusters', fontsize=14)\n",
    "plt.xlabel('N√∫mero de Clusters')\n",
    "plt.ylabel('Score de Silhueta')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(r\"C:\\Users\\CWS\\Documents\\meu\\Meu tcc\\clusters\\silhouette_method_improved.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(f\"M√©todo da Silhueta sugere: {optimal_silhouette} clusters\")\n",
    "\n",
    "# M√©trica Davies-Bouldin\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "db_scores = []\n",
    "for i in range(2, 11):\n",
    "    kmeans = KMeans(n_clusters=i, random_state=42)\n",
    "    labels = kmeans.fit_predict(X)\n",
    "    db_scores.append(davies_bouldin_score(X, labels))\n",
    "\n",
    "optimal_db = range(2, 11)[np.argmin(db_scores)]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(2, 11), db_scores, marker='o')\n",
    "plt.axvline(x=optimal_db, color='b', linestyle='--',\n",
    "            label=f'√ìtimo sugerido (k={optimal_db})')\n",
    "\n",
    "plt.title('M√©todo Davies-Bouldin - N√∫mero √ìtimo de Clusters', fontsize=18)\n",
    "plt.xlabel('N√∫mero de Clusters', fontsize=16)\n",
    "plt.ylabel('Davies-Bouldin Score (menor √© melhor)', fontsize=16)\n",
    "plt.legend(fontsize=16)\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "\n",
    "plt.grid(True)\n",
    "plt.savefig(r\"C:\\Users\\CWS\\Documents\\meu\\Meu tcc\\clusters\\davies_bouldin_method.png\",\n",
    "            dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "print(f\"M√©todo Davies-Bouldin sugere: {optimal_db} clusters\")\n",
    "\n",
    "# Op√ß√£o 1: Usar o valor da maioria\n",
    "# Definir 2 clusters conforme Silhueta e Davies-Bouldin\n",
    "n_clusters = 2\n",
    "\n",
    "# Aplicar K-means\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "df_cluster['cluster'] = kmeans.fit_predict(X)\n",
    "\n",
    "# Adicionar informa√ß√µes geogr√°ficas de volta\n",
    "df_cluster['SG_UF_ESC'] = df_nordeste.loc[df_cluster.index, 'SG_UF_ESC']\n",
    "\n",
    "# Testar tamb√©m com 3 clusters para compara√ß√£o\n",
    "kmeans_k3 = KMeans(n_clusters=3, random_state=42)\n",
    "labels_k3 = kmeans_k3.fit_predict(X)\n",
    "centroids_k3 = kmeans_k3.cluster_centers_\n",
    "\n",
    "# M√©tricas para k=2 (j√° calculado)\n",
    "silh_k2 = silhouette_score(X, df_cluster['cluster']) \n",
    "# M√©tricas para k=3\n",
    "silh_k3 = silhouette_score(X, labels_k3)\n",
    "\n",
    "print(f\"\\nCompara√ß√£o de m√©todos:\")\n",
    "print(f\"Silhueta k=2: {silh_k2:.3f} | Silhueta k=3: {silh_k3:.3f}\")\n",
    "print(f\"Rela√ß√£o de qualidade: {(silh_k3/silh_k2)*100:.1f}%\")\n",
    "\n",
    "# Calcular R¬≤ para k=3\n",
    "inter_cluster_dist_k3 = pairwise_distances(centroids_k3)\n",
    "intra_cluster_dist_k3 = [np.mean(pairwise_distances(X[labels_k3==i])) for i in range(3)]\n",
    "r_squared_k3 = (inter_cluster_dist_k3.mean() - np.mean(intra_cluster_dist_k3)) / inter_cluster_dist_k3.mean()\n",
    "\n",
    "# Calcular R¬≤ para k=2\n",
    "inter_cluster_dist_k2 = pairwise_distances(kmeans.cluster_centers_)\n",
    "intra_cluster_dist_k2 = [np.mean(pairwise_distances(X[df_cluster['cluster']==i])) for i in range(2)]\n",
    "r_squared_k2 = (inter_cluster_dist_k2.mean() - np.mean(intra_cluster_dist_k2)) / inter_cluster_dist_k2.mean()\n",
    "\n",
    "print(f\"R¬≤ entre clusters k=2: {r_squared_k2:.3f} | k=3: {r_squared_k3:.3f}\")\n",
    "print(f\"Ganho explicativo: {(r_squared_k3/r_squared_k2-1)*100:.1f}%\")\n",
    "\n",
    "\n",
    "# ETAPA 4: P√≥s-clusteriza√ß√£o (ap√≥s kmeans.fit_predict)\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# Renomear os clusters para an√°lise interpret√°vel\n",
    "cluster_names_2 = {\n",
    "    0: 'Vulner√°vel',\n",
    "    1: 'Privilegiado'\n",
    "}\n",
    "\n",
    "# Criar nova coluna com os nomes dos clusters\n",
    "df_cluster['cluster_nome'] = df_cluster['cluster'].map(cluster_names)\n",
    "\n",
    "# Atribuir nomes leg√≠veis\n",
    "df_cluster['cluster_nome'] = df_cluster['cluster'].map(cluster_names_2)\n",
    "df_cluster['cluster'] = pd.Categorical(\n",
    "    df_cluster['cluster_nome'],\n",
    "    categories=['Vulner√°vel', 'Privilegiado'],\n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "# -----------------------------\n",
    "# ETAPA 5: Visualiza√ß√£o dos Clusters\n",
    "# -----------------------------\n",
    "\n",
    "# PCA para vari√¢ncia explicada acumulada\n",
    "pca_temp = PCA().fit(X)\n",
    "explained = np.cumsum(pca_temp.explained_variance_ratio_)\n",
    "\n",
    "# Determinar quantos componentes atingem 80% de vari√¢ncia\n",
    "n_componentes_80 = np.argmax(explained >= 0.80) + 1\n",
    "print(f\"Componentes para 80% da vari√¢ncia: {n_componentes_80}\")\n",
    "\n",
    "\n",
    "# Aplicar PCA com esse n√∫mero\n",
    "pca = PCA(n_components=n_componentes_80)\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Gr√°fico 3D interativo\n",
    "fig = px.scatter_3d(\n",
    "    x=X_pca[:, 0], y=X_pca[:, 1], z=X_pca[:, 2],  # Agora todas as 3 dimens√µes existem\n",
    "    color=df_cluster['cluster'],\n",
    "    labels={'color': 'Cluster'},\n",
    "    title='Visualiza√ß√£o 3D dos Clusters (2 grupos)'\n",
    ")\n",
    "fig.write_html(r\"C:\\Users\\CWS\\Documents\\meu\\Meu tcc\\clusters\\3d_clusters_2.html\")\n",
    "\n",
    "# Gr√°fico de dispers√£o 2D\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(\n",
    "    x=X_pca[:, 0], y=X_pca[:, 1],\n",
    "    hue=df_cluster['cluster'],\n",
    "    palette='Set2', s=60, alpha=0.7\n",
    ")\n",
    "\n",
    "plt.title('Visualiza√ß√£o dos Clusters (PCA)', fontsize=20)\n",
    "plt.xlabel('Componente Principal 1', fontsize=18)\n",
    "plt.ylabel('Componente Principal 2', fontsize=18)\n",
    "\n",
    "plt.xticks(fontsize=16)\n",
    "plt.yticks(fontsize=16)\n",
    "plt.legend(title='Cluster', fontsize=16, title_fontsize=18)\n",
    "\n",
    "plt.savefig(r\"C:\\Users\\CWS\\Documents\\meu\\Meu tcc\\clusters\\pca_clusters_2.png\",\n",
    "            dpi=300, bbox_inches=\"tight\")\n",
    "plt.close()\n",
    "\n",
    "# Vari√¢ncia explicada\n",
    "explained_variance = pca.explained_variance_ratio_\n",
    "cumulative_variance = np.cumsum(explained_variance)\n",
    "\n",
    "print(\"\\nVari√¢ncia explicada pelas componentes PCA:\")\n",
    "print(f\"PC1: {explained_variance[0]:.2%}\")\n",
    "print(f\"PC2: {explained_variance[1]:.2%} (Acumulada: {cumulative_variance[1]:.2%})\")\n",
    "print(f\"PC3: {explained_variance[2]:.2%} (Acumulada: {cumulative_variance[2]:.2%})\")\n",
    "\n",
    "# Plot vari√¢ncia explicada\n",
    "componentes = len(explained_variance)\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.bar(range(1, componentes + 1), explained_variance, alpha=0.6, label='Individual')\n",
    "plt.plot(range(1, componentes + 1), cumulative_variance, marker='o', label='Acumulada')\n",
    "plt.xlabel('Componentes Principais')\n",
    "plt.ylabel('Vari√¢ncia Explicada')\n",
    "plt.title('Vari√¢ncia Explicada pelas Componentes PCA', fontsize=14)\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(r\"C:\\Users\\CWS\\Documents\\meu\\Meu tcc\\clusters\\pca_variance.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# ETAPA 6: An√°lise dos Clusters\n",
    "# -----------------------------\n",
    "\n",
    "# Caracteriza√ß√£o dos clusters\n",
    "cluster_stats = df_cluster.groupby('cluster').agg({\n",
    "    'Q001': 'mean',            # Escolaridade do pai\n",
    "    'Q002': 'mean',            # Escolaridade da m√£e\n",
    "    'Q003': 'mean',            # Ocupa√ß√£o do pai\n",
    "    'Q004': 'mean',            # Ocupa√ß√£o da m√£e\n",
    "    'Q006': 'mean',            # Renda familiar\n",
    "    'Q024': 'mean',            # Computador em casa\n",
    "    'Q025': 'mean',            # Internet em casa\n",
    "    'TP_ESCOLA': 'mean',       # Tipo de escola\n",
    "    'TP_COR_RACA': 'mean',     # Ra√ßa/cor\n",
    "    'NU_NOTA_MEDIA': 'mean'    # Desempenho acad√™mico\n",
    "})\n",
    "print(\"\\nEstat√≠sticas por Cluster (2  clusters):\")\n",
    "print(cluster_stats.round(2))\n",
    "\n",
    "# Salvar estat√≠sticas com separador de v√≠rgula e ponto decimal\n",
    "cluster_stats.to_csv(\n",
    "    r\"C:\\Users\\CWS\\Documents\\meu\\Meu tcc\\clusters\\cluster_stats_2.csv\",\n",
    "    sep=',',\n",
    "    decimal='.',\n",
    "    index=True\n",
    ")\n",
    "\n",
    "\n",
    "# -----------------------------\n",
    "# ETAPA 7: Visualiza√ß√£o por Estado\n",
    "# -----------------------------\n",
    "\n",
    "# -----------------------------\n",
    "# ETAPA 7: Visualiza√ß√£o por Estado - C√ìDIGO CORRIGIDO\n",
    "# -----------------------------\n",
    "\n",
    "# 1. Gr√°fico de barras (distribui√ß√£o proporcional por estado)\n",
    "cluster_por_estado = pd.crosstab(df_cluster['SG_UF_ESC'], df_cluster['cluster_nome'], normalize='index')\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "cluster_por_estado[['Vulner√°vel', 'Privilegiado']].plot(\n",
    "    kind='bar', \n",
    "    stacked=True, \n",
    "    color=['#d73027', '#fee08b', '#1a9850']\n",
    ")\n",
    "plt.title('Distribui√ß√£o Proporcional dos Clusters por Estado', fontsize=14)\n",
    "plt.xlabel('Estado')\n",
    "plt.ylabel('Propor√ß√£o')\n",
    "plt.legend(title='Cluster', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig(r\"C:\\Users\\CWS\\Documents\\meu\\Meu tcc\\clusters\\clusters_por_estado_2.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# 2. Mapa coropl√©tico (cluster predominante por estado)\n",
    "estados = read_state(year=2020)\n",
    "\n",
    "# Verificar nome da coluna de siglas\n",
    "print(\"\\nColunas dispon√≠veis no geodataframe:\", estados.columns.tolist())\n",
    "\n",
    "# O nome correto da coluna pode variar - ajuste conforme necess√°rio\n",
    "coluna_sigla = 'abbrev_state'  # ou 'sigla' dependendo da vers√£o do geobr\n",
    "\n",
    "estados_nordeste = estados[estados[coluna_sigla].isin(ufs_nordeste)].copy()\n",
    "\n",
    "\n",
    "# Calcular cluster predominante\n",
    "cluster_predominante = df_cluster.groupby('SG_UF_ESC')['cluster_nome'].agg(\n",
    "    lambda x: x.value_counts().index[0]\n",
    ").reset_index()\n",
    "\n",
    "# Juntar com o mapa\n",
    "estados_nordeste = estados_nordeste.merge(\n",
    "    cluster_predominante.rename(columns={'SG_UF_ESC': coluna_sigla}),\n",
    "    on=coluna_sigla,\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Definir ordem e cores\n",
    "# Cores e ordem\n",
    "ordem_clusters_2 = ['Vulner√°vel', 'Privilegiado']\n",
    "cores_clusters_2 = ['#d73027', '#1a9850']\n",
    "cmap_2 = ListedColormap(cores_clusters_2)\n",
    "\n",
    "estados_nordeste['cluster'] = pd.Categorical(\n",
    "    estados_nordeste['cluster_nome'],\n",
    "    categories=ordem_clusters_2,\n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "# Plotar o mapa\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "estados_nordeste.plot(\n",
    "    column='cluster',\n",
    "    categorical=True,\n",
    "    cmap=cmap_clusters,\n",
    "    edgecolor='black',\n",
    "    linewidth=0.8,\n",
    "    legend=True,\n",
    "    ax=ax,\n",
    "    missing_kwds={'color': 'lightgrey'}\n",
    ")\n",
    "\n",
    "# Ajustar legenda\n",
    "legenda = ax.get_legend()\n",
    "if legenda:\n",
    "    legenda.set_title('Perfil Socioecon√¥mico')\n",
    "    for texto, rotulo in zip(legenda.get_texts(), ordem_clusters):\n",
    "        texto.set_text(rotulo)\n",
    "\n",
    "# Adicionar r√≥tulos\n",
    "for idx, row in estados_nordeste.iterrows():\n",
    "    centroid = row.geometry.centroid\n",
    "    ax.annotate(\n",
    "        text=row[coluna_sigla],\n",
    "        xy=(centroid.x, centroid.y),\n",
    "        ha='center',\n",
    "        va='center',\n",
    "        fontsize=10,\n",
    "        bbox=dict(boxstyle='round,pad=0.3', fc='white', ec='none', alpha=0.8)\n",
    "    )\n",
    "\n",
    "ax.set_title('Perfil Socioecon√¥mico Predominante por Estado no Nordeste', fontsize=14)\n",
    "ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.savefig(r\"C:\\Users\\CWS\\Documents\\meu\\Meu tcc\\clusters\\mapa_clusters_final_2.png\", dpi=300, bbox_inches='tight')\n",
    "plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "eece218b-cb9c-449f-92bd-791e91b06ccd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Capitais n√£o mapeadas: []\n",
      "Limiar 2: 30.75% das amostras seriam removidas.\n",
      "Limiar 2.5: 19.47% das amostras seriam removidas.\n",
      "Limiar 3: 10.29% das amostras seriam removidas.\n",
      "Limiar 3.5: 7.16% das amostras seriam removidas.\n",
      "\n",
      "M√©todo do Cotovelo sugere: 9 clusters\n",
      "M√©todo da Silhueta sugere: 2 clusters\n",
      "M√©todo Davies-Bouldin sugere: 2 clusters\n",
      "\n",
      "Estat√≠sticas por Cluster (3 clusters):\n",
      "               Q001  Q002  Q003  Q004   Q006  Q024  Q025  TP_ESCOLA  \\\n",
      "cluster_nome                                                          \n",
      "Intermedi√°rio  3.00  3.35  2.08  1.84   2.14  0.35   1.0       0.14   \n",
      "Privilegiado   3.71  4.06  2.39  2.19   3.43  0.68   1.0       0.41   \n",
      "Vulner√°vel     5.11  5.31  3.46  3.37  10.44  2.12   1.0       0.92   \n",
      "\n",
      "               TP_COR_RACA  NU_NOTA_MEDIA  \n",
      "cluster_nome                               \n",
      "Intermedi√°rio         0.23         478.96  \n",
      "Privilegiado          0.37         601.72  \n",
      "Vulner√°vel            0.69         675.68  \n",
      "\n",
      "M√©dia das Notas por √Årea do Conhecimento (por Cluster):\n",
      "               Ci√™ncias da Natureza  Ci√™ncias Humanas  Linguagens e C√≥digos  \\\n",
      "cluster_nome                                                                  \n",
      "Vulner√°vel                   595.05            619.16                597.14   \n",
      "Intermedi√°rio                446.33            464.03                471.42   \n",
      "Privilegiado                 522.64            565.30                554.48   \n",
      "\n",
      "               Matem√°tica  Reda√ß√£o  \n",
      "cluster_nome                        \n",
      "Vulner√°vel         711.42   855.65  \n",
      "Intermedi√°rio      453.44   559.58  \n",
      "Privilegiado       593.12   773.06  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 1200x800 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.metrics import silhouette_score\n",
    "import plotly.express as px\n",
    "from geobr import read_state\n",
    "from matplotlib.colors import ListedColormap\n",
    "import unicodedata\n",
    "from scipy.stats import zscore\n",
    "\n",
    "# Configura√ß√µes iniciais\n",
    "plt.style.use('ggplot')\n",
    "sns.set_palette(\"Set2\")\n",
    "pd.set_option('display.max_columns', None)\n",
    "\n",
    "# -----------------------------\n",
    "# ETAPA 1: Carregar e preparar os dados\n",
    "# -----------------------------\n",
    "# Fun√ß√£o para remover acentos e padronizar\n",
    "def normalizar_texto(texto):\n",
    "    if pd.isnull(texto):\n",
    "        return texto\n",
    "    texto = str(texto).strip().title()\n",
    "    texto = unicodedata.normalize('NFKD', texto).encode('ASCII', 'ignore').decode('utf-8')\n",
    "    return texto\n",
    "\n",
    "# Dicion√°rio: capitais do Nordeste ‚Üí siglas dos estados\n",
    "capitais_para_uf = {\n",
    "    'S√£o Lu√≠s': 'MA',\n",
    "    'Teresina': 'PI',\n",
    "    'Fortaleza': 'CE',\n",
    "    'Natal': 'RN',\n",
    "    'Jo√£o Pessoa': 'PB',\n",
    "    'Recife': 'PE',\n",
    "    'Macei√≥': 'AL',\n",
    "    'Aracaju': 'SE',\n",
    "    'Salvador': 'BA'\n",
    "}\n",
    "\n",
    "# Ajustar o dicion√°rio tamb√©m para ter nomes sem acento\n",
    "capitais_para_uf_normalizado = {normalizar_texto(k): v for k, v in capitais_para_uf.items()}\n",
    "\n",
    "# Carregar dados do ENEM (ajuste o caminho)\n",
    "caminho_arquivo = r\"C:\\Users\\CWS\\Documents\\meu\\Meu tcc\\microdados_enem_tratados2.csv\"\n",
    "df = pd.read_csv(caminho_arquivo, sep=\";\", encoding=\"utf-8\")\n",
    "\n",
    "df['NO_MUNICIPIO_PROVA_NORMALIZADO'] = df['NO_MUNICIPIO_PROVA'].apply(normalizar_texto)\n",
    "\n",
    "# Mapeia sigla da UF a partir da capital normalizada\n",
    "capitais_para_uf_normalizado = {normalizar_texto(k): v for k, v in capitais_para_uf.items()}\n",
    "df['SG_UF_ESC'] = df['NO_MUNICIPIO_PROVA_NORMALIZADO'].map(capitais_para_uf_normalizado)\n",
    "\n",
    "faltantes = df[df['SG_UF_ESC'].isnull()]['NO_MUNICIPIO_PROVA'].unique()\n",
    "print(\"Capitais n√£o mapeadas:\", faltantes)\n",
    "\n",
    "# Filtrar apenas alunos do Nordeste\n",
    "ufs_nordeste = ['MA', 'PI', 'CE', 'RN', 'PB', 'PE', 'AL', 'SE', 'BA']\n",
    "df_nordeste = df[df['SG_UF_ESC'].isin(ufs_nordeste)].copy()\n",
    "\n",
    "# Calcular nota m√©dia\n",
    "df_nordeste['NU_NOTA_MEDIA'] = df_nordeste[['NU_NOTA_CN', 'NU_NOTA_CH', 'NU_NOTA_LC', 'NU_NOTA_MT', 'NU_NOTA_REDACAO']].mean(axis=1)\n",
    "\n",
    "# Selecionar vari√°veis para clusteriza√ß√£o\n",
    "variaveis = [\n",
    "    'Q001',  # Escolaridade do pai\n",
    "    'Q002',  # Escolaridade da m√£e\n",
    "    'Q003',  # Ocupa√ß√£o do pai\n",
    "    'Q004',  # Ocupa√ß√£o da m√£e\n",
    "    'Q006',  # Renda familiar\n",
    "    'Q024',  # Computador em casa\n",
    "    'Q025',  # Internet em casa\n",
    "    'TP_ESCOLA',  # Tipo de escola\n",
    "    'TP_COR_RACA',  # Ra√ßa/cor\n",
    "    'NU_NOTA_MEDIA',  # Desempenho acad√™mico\n",
    "    'NU_NOTA_CN', 'NU_NOTA_CH', 'NU_NOTA_LC', 'NU_NOTA_MT', 'NU_NOTA_REDACAO' \n",
    "]\n",
    "\n",
    "df_cluster = df_nordeste[variaveis].dropna().copy()\n",
    "\n",
    "# -----------------------------\n",
    "# ETAPA 2: Pr√©-processamento\n",
    "# -----------------------------\n",
    "\n",
    "# Codificar vari√°veis categ√≥ricas\n",
    "encoder = LabelEncoder()\n",
    "df_cluster['TP_ESCOLA'] = encoder.fit_transform(df_cluster['TP_ESCOLA'])\n",
    "df_cluster['TP_COR_RACA'] = encoder.fit_transform(df_cluster['TP_COR_RACA'])\n",
    "\n",
    "# Selecionar colunas num√©ricas para clusteriza√ß√£o\n",
    "X = df_cluster.select_dtypes(include=[np.number]).values\n",
    "\n",
    "\n",
    "# Verificar porcentagem de outliers para diferentes limiares\n",
    "for limite in [2, 2.5, 3, 3.5]:\n",
    "    outliers = (np.abs(zscore(X)) > limite).any(axis=1)\n",
    "    porcentagem = 100 * outliers.sum() / len(X)\n",
    "    print(f\"Limiar {limite}: {porcentagem:.2f}% das amostras seriam removidas.\")\n",
    "    \n",
    "\n",
    "# Definir limiar final e remover outliers\n",
    "limite_zscore = 3\n",
    "z_scores = np.abs(zscore(X))\n",
    "filtro_sem_outliers = (z_scores < limite_zscore).all(axis=1)\n",
    "X_sem_outliers = X[filtro_sem_outliers]\n",
    "df_cluster_sem_outliers = df_cluster[filtro_sem_outliers].copy()\n",
    "\n",
    "\n",
    "# Normalizar os dados ap√≥s remo√ß√£o de outliers\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(df_cluster_sem_outliers)\n",
    "# -----------------------------\n",
    "# ETAPA 3: Determinar n√∫mero √≥timo de clusters\n",
    "# -----------------------------\n",
    "\n",
    "range_clusters = range(1, 11)  # Definir o range de clusters a testar\n",
    "# -----------------------------\n",
    "\n",
    "# M√©todo do cotovelo (melhorado)\n",
    "wcss = []\n",
    "for i in range_clusters:\n",
    "    kmeans = KMeans(n_clusters=i, random_state=42)\n",
    "    kmeans.fit(X)\n",
    "    wcss.append(kmeans.inertia_)\n",
    "\n",
    "# Identificar o \"cotovelo\" programaticamente\n",
    "elbow_diff = np.diff(wcss)\n",
    "elbow_angle = np.diff(elbow_diff)\n",
    "optimal_elbow = np.argmin(elbow_angle) + 2  # +2 porque diff reduz em 2 o tamanho\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range_clusters, wcss, marker='o', label='WCSS')\n",
    "plt.axvline(x=optimal_elbow, color='r', linestyle='--', \n",
    "            label=f'Cotovelo sugerido (k={optimal_elbow})')\n",
    "plt.title('M√©todo do Cotovelo - N√∫mero √ìtimo de Clusters', fontsize=14)\n",
    "plt.xlabel('N√∫mero de Clusters')\n",
    "plt.ylabel('WCSS (In√©rcia)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(r\"C:\\Users\\CWS\\Documents\\meu\\Meu tcc\\clusters\\elbow_method_improved.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(f\"\\nM√©todo do Cotovelo sugere: {optimal_elbow} clusters\")\n",
    "\n",
    "# M√©todo da silhueta (melhorado)\n",
    "silhouette_scores = []\n",
    "range_silhouette = range(2, 11)  # Silhueta requer pelo menos 2 clusters\n",
    "\n",
    "for i in range_silhouette:\n",
    "    kmeans = KMeans(n_clusters=i, random_state=42)\n",
    "    labels = kmeans.fit_predict(X)\n",
    "    silhouette_scores.append(silhouette_score(X, labels))\n",
    "\n",
    "optimal_silhouette = range_silhouette[np.argmax(silhouette_scores)]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range_silhouette, silhouette_scores, marker='o', label='Score de Silhueta')\n",
    "plt.axvline(x=optimal_silhouette, color='g', linestyle='--', \n",
    "            label=f'√ìtimo sugerido (k={optimal_silhouette})')\n",
    "plt.title('M√©todo da Silhueta - N√∫mero √ìtimo de Clusters', fontsize=14)\n",
    "plt.xlabel('N√∫mero de Clusters')\n",
    "plt.ylabel('Score de Silhueta')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(r\"C:\\Users\\CWS\\Documents\\meu\\Meu tcc\\clusters\\silhouette_method_improved.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(f\"M√©todo da Silhueta sugere: {optimal_silhouette} clusters\")\n",
    "\n",
    "# M√©trica Davies-Bouldin\n",
    "from sklearn.metrics import davies_bouldin_score\n",
    "\n",
    "db_scores = []\n",
    "for i in range(2, 11):\n",
    "    kmeans = KMeans(n_clusters=i, random_state=42)\n",
    "    labels = kmeans.fit_predict(X)\n",
    "    db_scores.append(davies_bouldin_score(X, labels))\n",
    "\n",
    "optimal_db = range(2, 11)[np.argmin(db_scores)]\n",
    "\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(range(2, 11), db_scores, marker='o', label='Davies-Bouldin Score')\n",
    "plt.axvline(x=optimal_db, color='b', linestyle='--', \n",
    "            label=f'√ìtimo sugerido (k={optimal_db})')\n",
    "plt.title('M√©todo Davies-Bouldin - N√∫mero √ìtimo de Clusters', fontsize=14)\n",
    "plt.xlabel('N√∫mero de Clusters')\n",
    "plt.ylabel('Davies-Bouldin Score (menor √© melhor)')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(r\"C:\\Users\\CWS\\Documents\\meu\\Meu tcc\\clusters\\davies_bouldin_method.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "print(f\"M√©todo Davies-Bouldin sugere: {optimal_db} clusters\")\n",
    "\n",
    "\n",
    "# Definir n√∫mero de clusters (ajuste conforme an√°lise acima)\n",
    "n_clusters = 3\n",
    "\n",
    "# Aplicar K-means apenas aos dados sem outliers\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "clusters = kmeans.fit_predict(X)  # X j√° est√° sem outliers\n",
    "\n",
    "# Criar nova coluna APENAS nas linhas sem outliers\n",
    "df_cluster_sem_outliers['cluster'] = clusters  # Use o dataframe filtrado\n",
    "\n",
    "# Adicionar informa√ß√µes geogr√°ficas corretamente\n",
    "df_cluster_sem_outliers['SG_UF_ESC'] = df_nordeste.loc[df_cluster_sem_outliers.index, 'SG_UF_ESC']\n",
    "\n",
    "# ETAPA 4: P√≥s-clusteriza√ß√£o (ap√≥s kmeans.fit_predict)\n",
    "# ---------------------------------------------------\n",
    "\n",
    "# Renomear os clusters para an√°lise interpret√°vel\n",
    "cluster_names = {\n",
    "    0: 'Intermedi√°rio',\n",
    "    1: 'Vulner√°vel', \n",
    "    2: 'Privilegiado'\n",
    "}\n",
    "\n",
    "# Aplicar nos dados SEM OUTLIERS\n",
    "df_cluster_sem_outliers['cluster_nome'] = df_cluster_sem_outliers['cluster'].map(cluster_names)\n",
    "\n",
    "# Converter para categoria ordenada\n",
    "df_cluster_sem_outliers['cluster'] = pd.Categorical(\n",
    "    df_cluster_sem_outliers['cluster_nome'],\n",
    "    categories=['Vulner√°vel', 'Intermedi√°rio', 'Privilegiado'],\n",
    "    ordered=True\n",
    ")\n",
    "# -----------------------------\n",
    "# ETAPA 5: Visualiza√ß√£o dos Clusters\n",
    "# -----------------------------\n",
    "\n",
    "# Redu√ß√£o para 3 componentes\n",
    "pca = PCA(n_components=3)  # Alterado para 3\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Gr√°fico 3D interativo\n",
    "fig = px.scatter_3d(\n",
    "    x=X_pca[:, 0], y=X_pca[:, 1], z=X_pca[:, 2],  # Agora todas as 3 dimens√µes existem\n",
    "    color=df_cluster_sem_outliers['cluster'],\n",
    "    labels={'color': 'Cluster'},\n",
    "    title='Visualiza√ß√£o 3D dos Clusters'\n",
    ")\n",
    "fig.write_html(r\"C:\\Users\\CWS\\Documents\\meu\\Meu tcc\\clusters\\3d_clusters.html\")\n",
    "\n",
    "# Gr√°fico de dispers√£o 2D\n",
    "plt.figure(figsize=(12, 8))\n",
    "sns.scatterplot(x=X_pca[:, 0], y=X_pca[:, 1], hue= df_cluster_sem_outliers['cluster'], palette='Set2', s=50, alpha=0.7)\n",
    "plt.title('Visualiza√ß√£o dos Clusters (PCA)', fontsize=14)\n",
    "plt.xlabel('Componente Principal 1')\n",
    "plt.ylabel('Componente Principal 2')\n",
    "plt.legend(title='Cluster')\n",
    "plt.savefig(r\"C:\\Users\\CWS\\Documents\\meu\\Meu tcc\\clusters\\pca_clusters.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# ETAPA 6: An√°lise dos Clusters\n",
    "# -----------------------------\n",
    "\n",
    "# Caracteriza√ß√£o dos clusters\n",
    "\n",
    "cluster_stats = df_cluster_sem_outliers.groupby('cluster_nome').agg({\n",
    "    'Q001': 'mean',            # Escolaridade do pai\n",
    "    'Q002': 'mean',            # Escolaridade da m√£e\n",
    "    'Q003': 'mean',            # Ocupa√ß√£o do pai\n",
    "    'Q004': 'mean',            # Ocupa√ß√£o da m√£e\n",
    "    'Q006': 'mean',            # Renda familiar\n",
    "    'Q024': 'mean',            # Computador em casa\n",
    "    'Q025': 'mean',            # Internet em casa\n",
    "    'TP_ESCOLA': 'mean',       # Tipo de escola\n",
    "    'TP_COR_RACA': 'mean',     # Ra√ßa/cor\n",
    "    'NU_NOTA_MEDIA': 'mean'    # Desempenho acad√™mico\n",
    "})\n",
    "print(\"\\nEstat√≠sticas por Cluster (3 clusters):\")\n",
    "print(cluster_stats.round(2))\n",
    "\n",
    "# Salvar estat√≠sticas em formato compat√≠vel com Excel (PT-BR)\n",
    "cluster_stats.to_csv(\n",
    "    r\"C:\\Users\\CWS\\Documents\\meu\\Meu tcc\\clusters\\cluster_stats_3.csv\",\n",
    "    sep=';',        # separador de colunas\n",
    "    decimal=',',    # separador decimal\n",
    "    index=True\n",
    ")\n",
    "# -----------------------------\n",
    "# ETAPA 7: Visualiza√ß√£o por Estado (3 clusters)\n",
    "# -----------------------------\n",
    "\n",
    "# 1. Gr√°fico de barras (distribui√ß√£o proporcional por estado)\n",
    "cluster_por_estado = pd.crosstab(df_cluster_sem_outliers['SG_UF_ESC'], \n",
    "                                df_cluster_sem_outliers['cluster_nome'], \n",
    "                                normalize='index')\n",
    "\n",
    "plt.figure(figsize=(12, 8))\n",
    "cluster_por_estado[['Vulner√°vel', 'Intermedi√°rio', 'Privilegiado']].plot(\n",
    "    kind='bar', \n",
    "    stacked=True, \n",
    "    color=['#d73027', '#fee08b', '#1a9850']  # Cores para 3 clusters\n",
    ")\n",
    "plt.title('Distribui√ß√£o Proporcional dos Clusters por Estado', fontsize=14)\n",
    "plt.xlabel('Estado')\n",
    "plt.ylabel('Propor√ß√£o')\n",
    "plt.legend(title='Cluster', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "plt.tight_layout()\n",
    "plt.savefig(r\"C:\\Users\\CWS\\Documents\\meu\\Meu tcc\\clusters\\clusters_por_estado_3.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# 2. Mapa coropl√©tico (cluster predominante por estado)\n",
    "estados = read_state(year=2020)\n",
    "coluna_sigla = 'abbrev_state'  # Confirmar no seu ambiente\n",
    "\n",
    "estados_nordeste = estados[estados[coluna_sigla].isin(ufs_nordeste)].copy()\n",
    "\n",
    "# Calcular cluster predominante (agora com 3 categorias)\n",
    "cluster_predominante = df_cluster_sem_outliers.groupby('SG_UF_ESC')['cluster_nome'].agg(\n",
    "    lambda x: x.value_counts().index[0]\n",
    ").reset_index()\n",
    "\n",
    "# Juntar com o mapa\n",
    "estados_nordeste = estados_nordeste.merge(\n",
    "    cluster_predominante.rename(columns={'SG_UF_ESC': coluna_sigla}),\n",
    "    on=coluna_sigla,\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "# Definir ordem e cores para 3 clusters\n",
    "ordem_clusters = ['Vulner√°vel', 'Intermedi√°rio', 'Privilegiado']\n",
    "cores_clusters = ['#d73027', '#fee08b', '#1a9850']\n",
    "cmap_clusters = ListedColormap(cores_clusters)\n",
    "\n",
    "estados_nordeste['cluster'] = pd.Categorical(\n",
    "    estados_nordeste['cluster_nome'],\n",
    "    categories=ordem_clusters,\n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "# Plotar o mapa\n",
    "fig, ax = plt.subplots(figsize=(14, 10))\n",
    "estados_nordeste.plot(\n",
    "    column='cluster',\n",
    "    categorical=True,\n",
    "    cmap=cmap_clusters,\n",
    "    edgecolor='black',\n",
    "    linewidth=0.8,\n",
    "    legend=True,\n",
    "    ax=ax,\n",
    "    missing_kwds={'color': 'lightgrey'}\n",
    ")\n",
    "\n",
    "# Ajustar legenda\n",
    "legenda = ax.get_legend()\n",
    "if legenda:\n",
    "    legenda.set_title('Perfil Socioecon√¥mico')\n",
    "    for texto, rotulo in zip(legenda.get_texts(), ordem_clusters):\n",
    "        texto.set_text(rotulo)\n",
    "\n",
    "# Adicionar r√≥tulos dos estados\n",
    "for idx, row in estados_nordeste.iterrows():\n",
    "    centroid = row.geometry.centroid\n",
    "    ax.annotate(\n",
    "        text=row[coluna_sigla],\n",
    "        xy=(centroid.x, centroid.y),\n",
    "        ha='center',\n",
    "        va='center',\n",
    "        fontsize=10,\n",
    "        bbox=dict(boxstyle='round,pad=0.3', fc='white', ec='none', alpha=0.8)\n",
    "    )\n",
    "\n",
    "ax.set_title('Perfil Socioecon√¥mico Predominante por Estado no Nordeste', fontsize=14)\n",
    "ax.axis('off')\n",
    "plt.tight_layout()\n",
    "plt.savefig(r\"C:\\Users\\CWS\\Documents\\meu\\Meu tcc\\clusters\\mapa_clusters_final_3.png\", \n",
    "           dpi=300, bbox_inches='tight')\n",
    "plt.close() \n",
    "\n",
    "# ETAPA EXTRA: Nota M√©dia por √Årea do Conhecimento por Cluster\n",
    "# -------------------------------------------------------------\n",
    "\n",
    "# Selecionar apenas colunas relevantes e renomear\n",
    "df_notas_cluster = df_cluster_sem_outliers[[\n",
    "    'cluster_nome', \n",
    "    'NU_NOTA_CN', \n",
    "    'NU_NOTA_CH', \n",
    "    'NU_NOTA_LC', \n",
    "    'NU_NOTA_MT', \n",
    "    'NU_NOTA_REDACAO'\n",
    "]].copy()\n",
    "\n",
    "# Renomear colunas para nomes amig√°veis\n",
    "df_notas_cluster.rename(columns={\n",
    "    'NU_NOTA_CN': 'Ci√™ncias da Natureza',\n",
    "    'NU_NOTA_CH': 'Ci√™ncias Humanas',\n",
    "    'NU_NOTA_LC': 'Linguagens e C√≥digos',\n",
    "    'NU_NOTA_MT': 'Matem√°tica',\n",
    "    'NU_NOTA_REDACAO': 'Reda√ß√£o'\n",
    "}, inplace=True)\n",
    "\n",
    "# Calcular m√©dias por cluster\n",
    "notas_media_por_cluster = df_notas_cluster.groupby('cluster_nome').mean().round(2)\n",
    "\n",
    "# Ordenar pela ordem desejada dos clusters\n",
    "ordem_clusters = ['Vulner√°vel', 'Intermedi√°rio', 'Privilegiado']\n",
    "notas_media_por_cluster = notas_media_por_cluster.loc[ordem_clusters]\n",
    "\n",
    "# Exibir resultado\n",
    "print(\"\\nM√©dia das Notas por √Årea do Conhecimento (por Cluster):\")\n",
    "print(notas_media_por_cluster)\n",
    "\n",
    "# Exportar para CSV (compat√≠vel com Excel em PT-BR)\n",
    "notas_media_por_cluster.to_csv(\n",
    "    r\"C:\\Users\\CWS\\Documents\\meu\\Meu tcc\\clusters\\notas_media_por_cluster.csv\",\n",
    "    sep=';',     # separador de colunas para Excel PT-BR\n",
    "    decimal=',', # separador decimal\n",
    "    encoding='utf-8-sig'  # garante compatibilidade com Excel\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e79409dd-f2e2-4877-aad2-9d466c6a6d5e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Resultados da ANOVA para cada vari√°vel por Cluster ---\n",
      "Hip√≥tese Nula (H0): N√£o h√° diferen√ßa significativa nas m√©dias da vari√°vel entre os clusters.\n",
      "Hip√≥tese Alternativa (H1): H√° uma diferen√ßa significativa nas m√©dias da vari√°vel entre pelo menos dois clusters.\n",
      "Um p-valor menor que 0.05 (n√≠vel de signific√¢ncia comum) sugere rejeitar a H0.\n",
      "\n",
      "Vari√°vel: Q001\n",
      "  F-estat√≠stica: 19090.4825\n",
      "  P-valor: 0.0000\n",
      "  Conclus√£o: Rejeitar H0. H√° diferen√ßa estatisticamente significativa entre as m√©dias dos clusters.\n",
      "\n",
      "Vari√°vel: Q002\n",
      "  F-estat√≠stica: 20001.2276\n",
      "  P-valor: 0.0000\n",
      "  Conclus√£o: Rejeitar H0. H√° diferen√ßa estatisticamente significativa entre as m√©dias dos clusters.\n",
      "\n",
      "Vari√°vel: Q003\n",
      "  F-estat√≠stica: 12094.1702\n",
      "  P-valor: 0.0000\n",
      "  Conclus√£o: Rejeitar H0. H√° diferen√ßa estatisticamente significativa entre as m√©dias dos clusters.\n",
      "\n",
      "Vari√°vel: Q004\n",
      "  F-estat√≠stica: 14788.7529\n",
      "  P-valor: 0.0000\n",
      "  Conclus√£o: Rejeitar H0. H√° diferen√ßa estatisticamente significativa entre as m√©dias dos clusters.\n",
      "\n",
      "Vari√°vel: Q006\n",
      "  F-estat√≠stica: 58919.4024\n",
      "  P-valor: 0.0000\n",
      "  Conclus√£o: Rejeitar H0. H√° diferen√ßa estatisticamente significativa entre as m√©dias dos clusters.\n",
      "\n",
      "Vari√°vel: Q024\n",
      "  F-estat√≠stica: 31261.9730\n",
      "  P-valor: 0.0000\n",
      "  Conclus√£o: Rejeitar H0. H√° diferen√ßa estatisticamente significativa entre as m√©dias dos clusters.\n",
      "\n",
      "Vari√°vel: Q025\n",
      "  F-estat√≠stica: nan\n",
      "  P-valor: nan\n",
      "  Conclus√£o: N√£o rejeitar H0. N√£o h√° diferen√ßa estatisticamente significativa entre as m√©dias dos clusters.\n",
      "\n",
      "Vari√°vel: TP_ESCOLA\n",
      "  F-estat√≠stica: 27394.5002\n",
      "  P-valor: 0.0000\n",
      "  Conclus√£o: Rejeitar H0. H√° diferen√ßa estatisticamente significativa entre as m√©dias dos clusters.\n",
      "\n",
      "Vari√°vel: TP_COR_RACA\n",
      "  F-estat√≠stica: 4721.7344\n",
      "  P-valor: 0.0000\n",
      "  Conclus√£o: Rejeitar H0. H√° diferen√ßa estatisticamente significativa entre as m√©dias dos clusters.\n",
      "\n",
      "Vari√°vel: NU_NOTA_MEDIA\n",
      "  F-estat√≠stica: 14013.3262\n",
      "  P-valor: 0.0000\n",
      "  Conclus√£o: Rejeitar H0. H√° diferen√ßa estatisticamente significativa entre as m√©dias dos clusters.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\CWS\\anaconda3\\Lib\\site-packages\\scipy\\stats\\_axis_nan_policy.py:531: ConstantInputWarning: Each of the input arrays is constant; the F statistic is not defined or infinite\n",
      "  res = hypotest_fun_out(*samples, **kwds)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import unicodedata\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.stats import f_oneway\n",
    "\n",
    "# Function to remove accents and standardize\n",
    "def normalizar_texto(texto):\n",
    "    if pd.isnull(texto):\n",
    "        return texto\n",
    "    texto = str(texto).strip().title()\n",
    "    texto = unicodedata.normalize('NFKD', texto).encode('ASCII', 'ignore').decode('utf-8')\n",
    "    return texto\n",
    "\n",
    "# Dictionary: capitals of the Northeast -> state acronyms\n",
    "capitais_para_uf = {\n",
    "    'S√£o Lu√≠s': 'MA', 'Teresina': 'PI', 'Fortaleza': 'CE', 'Natal': 'RN',\n",
    "    'Jo√£o Pessoa': 'PB', 'Recife': 'PE', 'Macei√≥': 'AL', 'Aracaju': 'SE', 'Salvador': 'BA'\n",
    "}\n",
    "capitais_para_uf_normalizado = {normalizar_texto(k): v for k, v in capitais_para_uf.items()}\n",
    "\n",
    "# Load ENEM data (adjust path if needed, assuming it's accessible or a placeholder for the VM)\n",
    "# IMPORTANT: The user provided a local path C:\\Users\\CWS\\Documents\\TCC\\Meu tcc\\microdados_enem_tratados2.csv\n",
    "# This path is not accessible to the Python interpreter in the virtual environment.\n",
    "# I will assume the file is in the current working directory for demonstration, or will need to ask the user to upload it.\n",
    "# For now, I'll use a placeholder for `caminho_arquivo` and let the user know if the file isn't found.\n",
    "# If the file needs to be uploaded, the user will get an error about file not found, which will prompt them to upload.\n",
    "caminho_arquivo = r\"C:\\Users\\CWS\\Documents\\TCC\\Meu tcc\\microdados_enem_tratados2.csv\"\n",
    "\n",
    "try:\n",
    "    df = pd.read_csv(caminho_arquivo, sep=\";\", encoding=\"utf-8\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Erro: O arquivo '{caminho_arquivo}' n√£o foi encontrado. Por favor, certifique-se de que o arquivo est√° no diret√≥rio correto ou forne√ßa o caminho completo e acess√≠vel.\")\n",
    "    exit() # Exit the script if file not found\n",
    "\n",
    "df['NO_MUNICIPIO_PROVA_NORMALIZADO'] = df['NO_MUNICIPIO_PROVA'].apply(normalizar_texto)\n",
    "df['SG_UF_ESC'] = df['NO_MUNICIPIO_PROVA_NORMALIZADO'].map(capitais_para_uf_normalizado)\n",
    "\n",
    "ufs_nordeste = ['MA', 'PI', 'CE', 'RN', 'PB', 'PE', 'AL', 'SE', 'BA']\n",
    "df_nordeste = df[df['SG_UF_ESC'].isin(ufs_nordeste)].copy()\n",
    "\n",
    "df_nordeste['NU_NOTA_MEDIA'] = df_nordeste[['NU_NOTA_CN', 'NU_NOTA_CH', 'NU_NOTA_LC', 'NU_NOTA_MT', 'NU_NOTA_REDACAO']].mean(axis=1)\n",
    "\n",
    "variaveis = [\n",
    "    'Q001', 'Q002', 'Q003', 'Q004', 'Q006', 'Q024', 'Q025',\n",
    "    'TP_ESCOLA', 'TP_COR_RACA', 'NU_NOTA_MEDIA'\n",
    "]\n",
    "df_cluster = df_nordeste[variaveis].dropna().copy()\n",
    "\n",
    "# Encode categorical variables\n",
    "encoder = LabelEncoder()\n",
    "df_cluster['TP_ESCOLA'] = encoder.fit_transform(df_cluster['TP_ESCOLA'])\n",
    "df_cluster['TP_COR_RACA'] = encoder.fit_transform(df_cluster['TP_COR_RACA'])\n",
    "\n",
    "# Select numerical columns for clustering\n",
    "X = df_cluster.select_dtypes(include=[np.number]).values\n",
    "\n",
    "# Remove outliers using Z-score\n",
    "from scipy.stats import zscore\n",
    "limite_zscore = 3\n",
    "z_scores = np.abs(zscore(X))\n",
    "filtro_sem_outliers = (z_scores < limite_zscore).all(axis=1)\n",
    "X_sem_outliers = X[filtro_sem_outliers]\n",
    "df_cluster_sem_outliers = df_cluster[filtro_sem_outliers].copy()\n",
    "\n",
    "# Normalize the data after outlier removal\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(X_sem_outliers) # Renamed to X_scaled to avoid confusion with original X\n",
    "\n",
    "# Define number of clusters\n",
    "n_clusters = 3\n",
    "\n",
    "# Apply K-means to the scaled data\n",
    "kmeans = KMeans(n_clusters=n_clusters, random_state=42, n_init=10) # Added n_init to suppress warning\n",
    "clusters = kmeans.fit_predict(X_scaled)\n",
    "\n",
    "# Add cluster information to the filtered DataFrame\n",
    "df_cluster_sem_outliers['cluster'] = clusters\n",
    "df_cluster_sem_outliers['SG_UF_ESC'] = df_nordeste.loc[df_cluster_sem_outliers.index, 'SG_UF_ESC']\n",
    "\n",
    "# Rename clusters for interpretability\n",
    "cluster_names = {\n",
    "    0: 'Intermedi√°rio',\n",
    "    1: 'Vulner√°vel',\n",
    "    2: 'Privilegiado'\n",
    "}\n",
    "df_cluster_sem_outliers['cluster_nome'] = df_cluster_sem_outliers['cluster'].map(cluster_names)\n",
    "\n",
    "# Convert to ordered category\n",
    "df_cluster_sem_outliers['cluster'] = pd.Categorical(\n",
    "    df_cluster_sem_outliers['cluster_nome'],\n",
    "    categories=['Vulner√°vel', 'Intermedi√°rio', 'Privilegiado'],\n",
    "    ordered=True\n",
    ")\n",
    "\n",
    "# --- Apply ANOVA ---\n",
    "# Select numerical columns for ANOVA (excluding 'cluster' and 'cluster_nome' for direct stats)\n",
    "# We want to test the original variables against the cluster assignments\n",
    "numerical_cols_for_anova = [col for col in variaveis if col in df_cluster_sem_outliers.columns] # Ensure they are in the final df\n",
    "\n",
    "print(\"\\n--- Resultados da ANOVA para cada vari√°vel por Cluster ---\")\n",
    "print(\"Hip√≥tese Nula (H0): N√£o h√° diferen√ßa significativa nas m√©dias da vari√°vel entre os clusters.\")\n",
    "print(\"Hip√≥tese Alternativa (H1): H√° uma diferen√ßa significativa nas m√©dias da vari√°vel entre pelo menos dois clusters.\")\n",
    "print(\"Um p-valor menor que 0.05 (n√≠vel de signific√¢ncia comum) sugere rejeitar a H0.\")\n",
    "\n",
    "for col in numerical_cols_for_anova:\n",
    "    # Get data for each cluster\n",
    "    groups = [df_cluster_sem_outliers[df_cluster_sem_outliers['cluster_nome'] == c][col].dropna()\n",
    "              for c in df_cluster_sem_outliers['cluster_nome'].unique()]\n",
    "\n",
    "    # Ensure all groups have at least one observation\n",
    "    valid_groups = [g for g in groups if len(g) > 0]\n",
    "\n",
    "    if len(valid_groups) < 2:\n",
    "        print(f\"\\nVari√°vel: {col} - N√£o √© poss√≠vel realizar ANOVA, menos de 2 grupos com dados.\")\n",
    "        continue\n",
    "\n",
    "    # Perform one-way ANOVA\n",
    "    f_statistic, p_value = f_oneway(*valid_groups)\n",
    "\n",
    "    print(f\"\\nVari√°vel: {col}\")\n",
    "    print(f\"  F-estat√≠stica: {f_statistic:.4f}\")\n",
    "    print(f\"  P-valor: {p_value:.4f}\")\n",
    "\n",
    "    if p_value < 0.05:\n",
    "        print(\"  Conclus√£o: Rejeitar H0. H√° diferen√ßa estatisticamente significativa entre as m√©dias dos clusters.\")\n",
    "    else:\n",
    "        print(\"  Conclus√£o: N√£o rejeitar H0. N√£o h√° diferen√ßa estatisticamente significativa entre as m√©dias dos clusters.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09de599f-9176-4365-b747-0680edf6e6e0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:base] *",
   "language": "python",
   "name": "conda-base-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
